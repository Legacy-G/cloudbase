kubectl top pods -A
NAME                                STATUS   ROLES    AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
aks-nodepool1-74413310-vmss000000   Ready    <none>   18h   v1.32.7   10.224.0.5    <none>        Ubuntu 22.04.5 LTS   5.15.0-1096-azure   containerd://1.7.28-1
aks-nodepool1-74413310-vmss000001   Ready    <none>   18h   v1.32.7   10.224.0.4    <none>        Ubuntu 22.04.5 LTS   5.15.0-1096-azure   containerd://1.7.28-1
egrep: warning: egrep is obsolescent; using grep -E
                    kubernetes.azure.com/sku-cpu=2
                    kubernetes.azure.com/sku-memory=7168
  MemoryPressure                False   Thu, 06 Nov 2025 06:18:57 +0000   Wed, 05 Nov 2025 12:15:44 +0000   KubeletHasSufficientMemory      kubelet has sufficient memory available
Capacity:
  cpu:                2
  memory:             7097744Ki
Allocatable:
  cpu:                1900m
  memory:             5160336Ki
  Namespace                   Name                                                    CPU Requests  CPU Limits    Memory Requests  Memory Limits   Age
  cpu                1247m (65%)   16342m (860%)
  memory             2382Mi (47%)  33680Mi (668%)
                    kubernetes.azure.com/sku-cpu=2
                    kubernetes.azure.com/sku-memory=7168
  MemoryPressure                False   Thu, 06 Nov 2025 06:19:23 +0000   Wed, 05 Nov 2025 12:15:40 +0000   KubeletHasSufficientMemory      kubelet has sufficient memory available
Capacity:
  cpu:                2
  memory:             7097736Ki
Allocatable:
  cpu:                1900m
  memory:             5160328Ki
  Namespace                   Name                                                      CPU Requests  CPU Limits    Memory Requests  Memory Limits   Age
  cpu                1231m (64%)   22890m (1204%)
  memory             2686Mi (53%)  40943904Ki (793%)
NAMESPACE     NAME                                                     READY   STATUS    RESTARTS      AGE   IP             NODE                                NOMINATED NODE   READINESS GATES
kube-system   ama-logs-6kl88                                           3/3     Running   0             17h   10.244.0.94    aks-nodepool1-74413310-vmss000001   <none>           <none>
kube-system   ama-logs-rs-944b99f7d-hb7nl                              2/2     Running   0             17h   10.244.0.170   aks-nodepool1-74413310-vmss000001   <none>           <none>
kube-system   ama-logs-sdspk                                           3/3     Running   0             17h   10.244.1.26    aks-nodepool1-74413310-vmss000000   <none>           <none>
kube-system   ama-metrics-676b8547cc-jqd7k                             2/2     Running   0             16h   10.244.1.36    aks-nodepool1-74413310-vmss000000   <none>           <none>
kube-system   ama-metrics-676b8547cc-v9v2z                             2/2     Running   0             16h   10.244.0.3     aks-nodepool1-74413310-vmss000001   <none>           <none>
kube-system   ama-metrics-ksm-54c68f5cb5-z4qvx                         1/1     Running   0             16h   10.244.1.221   aks-nodepool1-74413310-vmss000000   <none>           <none>
kube-system   ama-metrics-node-hbhzk                                   2/2     Running   0             16h   10.244.0.164   aks-nodepool1-74413310-vmss000001   <none>           <none>
kube-system   ama-metrics-node-jkxp4                                   2/2     Running   0             16h   10.244.1.209   aks-nodepool1-74413310-vmss000000   <none>           <none>
kube-system   ama-metrics-operator-targets-6ff9b5d6d6-bx4bl            2/2     Running   3 (16h ago)   16h   10.244.0.35    aks-nodepool1-74413310-vmss000001   <none>           <none>
kube-system   azure-cns-j76lz                                          1/1     Running   0             18h   10.224.0.5     aks-nodepool1-74413310-vmss000000   <none>           <none>
kube-system   azure-cns-wt8gz                                          1/1     Running   0             18h   10.224.0.4     aks-nodepool1-74413310-vmss000001   <none>           <none>
kube-system   azure-ip-masq-agent-htrlx                                1/1     Running   0             18h   10.224.0.4     aks-nodepool1-74413310-vmss000001   <none>           <none>
kube-system   azure-ip-masq-agent-s5k86                                1/1     Running   0             18h   10.224.0.5     aks-nodepool1-74413310-vmss000000   <none>           <none>
kube-system   cloud-node-manager-8xc85                                 1/1     Running   0             18h   10.224.0.4     aks-nodepool1-74413310-vmss000001   <none>           <none>
kube-system   cloud-node-manager-mds69                                 1/1     Running   0             18h   10.224.0.5     aks-nodepool1-74413310-vmss000000   <none>           <none>
kube-system   coredns-6865d647c6-h2kj7                                 1/1     Running   0             18h   10.244.0.5     aks-nodepool1-74413310-vmss000001   <none>           <none>
kube-system   coredns-6865d647c6-tz7gn                                 1/1     Running   0             18h   10.244.1.248   aks-nodepool1-74413310-vmss000000   <none>           <none>
kube-system   coredns-autoscaler-67d9d668db-l9jzw                      1/1     Running   0             18h   10.244.0.212   aks-nodepool1-74413310-vmss000001   <none>           <none>
kube-system   csi-azuredisk-node-w7bpq                                 3/3     Running   0             18h   10.224.0.5     aks-nodepool1-74413310-vmss000000   <none>           <none>
kube-system   csi-azuredisk-node-wlsks                                 3/3     Running   0             18h   10.224.0.4     aks-nodepool1-74413310-vmss000001   <none>           <none>
kube-system   csi-azurefile-node-b8gwk                                 3/3     Running   0             18h   10.224.0.4     aks-nodepool1-74413310-vmss000001   <none>           <none>
kube-system   csi-azurefile-node-pt8qw                                 3/3     Running   0             18h   10.224.0.5     aks-nodepool1-74413310-vmss000000   <none>           <none>
kube-system   konnectivity-agent-7886bbb8f7-qscqj                      1/1     Running   0             17h   10.244.1.161   aks-nodepool1-74413310-vmss000000   <none>           <none>
kube-system   konnectivity-agent-7886bbb8f7-xn67l                      1/1     Running   0             17h   10.244.0.135   aks-nodepool1-74413310-vmss000001   <none>           <none>
kube-system   konnectivity-agent-autoscaler-6ff7779788-254xx           1/1     Running   0             18h   10.244.0.13    aks-nodepool1-74413310-vmss000001   <none>           <none>
kube-system   kube-proxy-db62g                                         1/1     Running   0             18h   10.224.0.4     aks-nodepool1-74413310-vmss000001   <none>           <none>
kube-system   kube-proxy-zvl8s                                         1/1     Running   0             18h   10.224.0.5     aks-nodepool1-74413310-vmss000000   <none>           <none>
kube-system   metrics-server-66b7768944-2w6xl                          2/2     Running   0             18h   10.244.1.173   aks-nodepool1-74413310-vmss000000   <none>           <none>
kube-system   metrics-server-66b7768944-5grtp                          2/2     Running   0             18h   10.244.1.120   aks-nodepool1-74413310-vmss000000   <none>           <none>
kube-system   retina-agent-2pg4q                                       1/1     Running   0             16h   10.224.0.5     aks-nodepool1-74413310-vmss000000   <none>           <none>
kube-system   retina-agent-69bqg                                       1/1     Running   0             16h   10.224.0.4     aks-nodepool1-74413310-vmss000001   <none>           <none>
monitoring    alertmanager-kube-prom-stack-kube-prome-alertmanager-0   2/2     Running   0             17h   10.244.0.163   aks-nodepool1-74413310-vmss000001   <none>           <none>
monitoring    kube-prom-stack-grafana-f5d67fcd6-jtpzf                  3/3     Running   0             17h   10.244.1.106   aks-nodepool1-74413310-vmss000000   <none>           <none>
monitoring    kube-prom-stack-kube-prome-operator-54dbf574c4-gq9jh     1/1     Running   0             17h   10.244.1.204   aks-nodepool1-74413310-vmss000000   <none>           <none>
monitoring    kube-prom-stack-kube-state-metrics-7b76d95fb4-6jxbz      1/1     Running   0             17h   10.244.0.186   aks-nodepool1-74413310-vmss000001   <none>           <none>
monitoring    kube-prom-stack-prometheus-node-exporter-hxhxt           0/1     Pending   0             17h   <none>         <none>                              <none>           <none>
monitoring    kube-prom-stack-prometheus-node-exporter-nj6gv           0/1     Pending   0             17h   <none>         <none>                              <none>           <none>
monitoring    prometheus-alertmanager-0                                1/1     Running   0             17h   10.244.1.207   aks-nodepool1-74413310-vmss000000   <none>           <none>
monitoring    prometheus-kube-prom-stack-kube-prome-prometheus-0       2/2     Running   0             17h   10.244.0.75    aks-nodepool1-74413310-vmss000001   <none>           <none>
monitoring    prometheus-kube-state-metrics-6598698c97-9wg6b           1/1     Running   0             17h   10.244.1.241   aks-nodepool1-74413310-vmss000000   <none>           <none>
monitoring    prometheus-prometheus-node-exporter-9mnn7                1/1     Running   0             17h   10.224.0.4     aks-nodepool1-74413310-vmss000001   <none>           <none>
monitoring    prometheus-prometheus-node-exporter-vczdj                1/1     Running   0             17h   10.224.0.5     aks-nodepool1-74413310-vmss000000   <none>           <none>
monitoring    prometheus-prometheus-pushgateway-6cddbcfff5-l92nq       1/1     Running   0             17h   10.244.0.72    aks-nodepool1-74413310-vmss000001   <none>           <none>
monitoring    prometheus-server-c568bf4db-44cs2                        2/2     Running   0             17h   10.244.0.41    aks-nodepool1-74413310-vmss000001   <none>           <none>
odoo-prod     db-8554889675-ldh8t                                      1/1     Running   0             17h   10.244.1.170   aks-nodepool1-74413310-vmss000000   <none>           <none>
odoo-prod     odoo-5d4577c586-s268p                                    1/1     Running   0             17h   10.244.0.33    aks-nodepool1-74413310-vmss000001   <none>           <none>
NAME                                CPU(cores)   CPU(%)   MEMORY(bytes)   MEMORY(%)   
aks-nodepool1-74413310-vmss000000   445m         23%      3527Mi          69%         
aks-nodepool1-74413310-vmss000001   321m         16%      4116Mi          81%         
NAMESPACE     NAME                                                     CPU(cores)   MEMORY(bytes)   
kube-system   ama-logs-6kl88                                           10m          279Mi           
kube-system   ama-logs-rs-944b99f7d-hb7nl                              6m           210Mi           
kube-system   ama-logs-sdspk                                           13m          262Mi           
kube-system   ama-metrics-676b8547cc-jqd7k                             13m          252Mi           
kube-system   ama-metrics-676b8547cc-v9v2z                             8m           210Mi           
kube-system   ama-metrics-ksm-54c68f5cb5-z4qvx                         3m           29Mi            
kube-system   ama-metrics-node-hbhzk                                   13m          289Mi           
kube-system   ama-metrics-node-jkxp4                                   15m          290Mi           
kube-system   ama-metrics-operator-targets-6ff9b5d6d6-bx4bl            3m           82Mi            
kube-system   azure-cns-j76lz                                          2m           46Mi            
kube-system   azure-cns-wt8gz                                          2m           51Mi            
kube-system   azure-ip-masq-agent-htrlx                                1m           16Mi            
kube-system   azure-ip-masq-agent-s5k86                                1m           23Mi            
kube-system   cloud-node-manager-8xc85                                 1m           17Mi            
kube-system   cloud-node-manager-mds69                                 1m           17Mi            
kube-system   coredns-6865d647c6-h2kj7                                 2m           32Mi            
kube-system   coredns-6865d647c6-tz7gn                                 2m           33Mi            
kube-system   coredns-autoscaler-67d9d668db-l9jzw                      1m           13Mi            
kube-system   csi-azuredisk-node-w7bpq                                 1m           88Mi            
kube-system   csi-azuredisk-node-wlsks                                 1m           58Mi            
kube-system   csi-azurefile-node-b8gwk                                 1m           66Mi            
kube-system   csi-azurefile-node-pt8qw                                 1m           71Mi            
kube-system   konnectivity-agent-7886bbb8f7-qscqj                      4m           29Mi            
kube-system   konnectivity-agent-7886bbb8f7-xn67l                      3m           26Mi            
kube-system   konnectivity-agent-autoscaler-6ff7779788-254xx           1m           13Mi            
kube-system   kube-proxy-db62g                                         1m           33Mi            
kube-system   kube-proxy-zvl8s                                         1m           46Mi            
kube-system   metrics-server-66b7768944-2w6xl                          3m           41Mi            
kube-system   metrics-server-66b7768944-5grtp                          3m           41Mi            
kube-system   retina-agent-2pg4q                                       1m           106Mi           
kube-system   retina-agent-69bqg                                       1m           108Mi           
monitoring    alertmanager-kube-prom-stack-kube-prome-alertmanager-0   1m           36Mi            
monitoring    kube-prom-stack-grafana-f5d67fcd6-jtpzf                  7m           324Mi           
monitoring    kube-prom-stack-kube-prome-operator-54dbf574c4-gq9jh     3m           24Mi            
monitoring    kube-prom-stack-kube-state-metrics-7b76d95fb4-6jxbz      2m           25Mi            
monitoring    prometheus-alertmanager-0                                2m           16Mi            
monitoring    prometheus-kube-prom-stack-kube-prome-prometheus-0       29m          341Mi           
monitoring    prometheus-kube-state-metrics-6598698c97-9wg6b           2m           24Mi            
monitoring    prometheus-prometheus-node-exporter-9mnn7                1m           13Mi            
monitoring    prometheus-prometheus-node-exporter-vczdj                1m           15Mi            
monitoring    prometheus-prometheus-pushgateway-6cddbcfff5-l92nq       1m           13Mi            
monitoring    prometheus-server-c568bf4db-44cs2                        5m           261Mi           
odoo-prod     db-8554889675-ldh8t                                      12m          259Mi           
odoo-prod     odoo-5d4577c586-s268p                                    2m           824Mi           
samuel [ ~ ]$ kubectl -n odoo-prod get svc odoo-svc -o wide
kubectl -n odoo-prod describe svc odoo-svc
NAME       TYPE           CLUSTER-IP     EXTERNAL-IP    PORT(S)                         AGE   SELECTOR
odoo-svc   LoadBalancer   10.0.174.167   4.253.33.191   8069:30210/TCP,8072:30876/TCP   17h   app=odoo
Name:                     odoo-svc
Namespace:                odoo-prod
Labels:                   <none>
Annotations:              <none>
Selector:                 app=odoo
Type:                     LoadBalancer
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.0.174.167
IPs:                      10.0.174.167
LoadBalancer Ingress:     4.253.33.191 (VIP)
Port:                     http  8069/TCP
TargetPort:               8069/TCP
NodePort:                 http  30210/TCP
Endpoints:                10.244.0.33:8069
Port:                     longpoll  8072/TCP
TargetPort:               8072/TCP
NodePort:                 longpoll  30876/TCP
Endpoints:                10.244.0.33:8072
Session Affinity:         None
External Traffic Policy:  Cluster
Internal Traffic Policy:  Cluster
Events:                   <none>
samuel [ ~ ]$ 


---
<!-- Code-Doc.md -->

# Get AKS credentials for kubectl access in the current shell session
az aks get-credentials \
  --resource-group cloudtrackit2025 \
  --name cloudtrack

# Get Nodes
kubectl get nodes

# Get Pods
kubectl get pods --all-namespaces

# Connect to the Azure Database for PostgreSQL using psql
export PGHOST=cloudtrack-db.postgres.database.azure.com
export PGUSER=postgres export PGPORT=5432
export PGDATABASE=postgres
export PGPASSWORD="Cloudtrack2025!"
psql

# Alternatively, connect in a single command
PGUSER="odoo" PGPASSWORD="Cloudtrack2025!" PGHOST="cloudt rack-db.postgres.database.azure.com" PGPORT=5432 PGDATABASE=postgres p sql "sslmode=require" psql

# To Apply Changes (Create Configurations)
kubectl apply -f dbms-aks.yaml
# or
kubectl apply -f dbms-postgres.yaml
# or
kubectl apply -f manifest.yaml
# or
kubectl apply -f deployment-manifest.yaml

# List all resources in the 'odoo-prod' namespace
kubectl -n odoo-prod get all

# Check logs of the PostgreSQL pod and the wait-for-postgres init container
kubectl -n odoo-prod logs -f pod/db-856ffbd5df-fv6gr
kubectl -n odoo-prod logs -f pod/odoo-54f6dfcd87-rxlst -c wait-for-postgres

# INITDB SCRIPTS CONFIGMAP CREATION AND USAGE

# LOCAL PREPARATION OF INITDB SCRIPT
# After creating initdb.sh file localll.........
# Convert the script locally to Unix format On your machine (before uploading to Cloud Shell):
dos2unix initdb.sh
chmod +x initdb.sh

# UPLOAD AND CREATE CONFIGMAP IN AZURE CLOUD SHELL
# Open Azure Cloud Shell in your browser.
# After that, upload the initdb.sh file to Azure Cloud Shell using the upload button in the Cloud Shell interface.

# Then Create initdb-scripts ConfigMap
kubectl -n odoo-prod create configmap initdb-scripts \
  --from-file=initdb.sh=initdb.sh

# Once done, Describe the initdb-scripts ConfigMap to verify its creation:
kubectl -n odoo-prod describe configmap initdb-scripts

# Restart the PostgreSQL pod to apply the initdb scripts
kubectl -n odoo-prod delete pod -l app=db

# Wait for the PostgreSQL pod to be Running again, then check roles and database
kubectl -n odoo-prod exec -it deploy/db -- psql -U postgres -c "\du"

# Delete initdb-scripts ConfigMap if needed or found something wrong with it
kubectl -n odoo-prod delete configmap initdb-scripts

# CHECKING THE DEPLOYMENT AND LOGS

# List all resources in the 'odoo-prod' namespace again
kubectl -n odoo-prod get all
# if not running, wait a bit and recheck or debug

# if yes, Confirm the odoo role exists in Postgres Exec into the Postgres pod:
kubectl -n odoo-prod exec -it deploy/db -- psql -U postgres

# Restart the Odoo deployment to apply any changes
kubectl -n odoo-prod rollout restart deploy/odoo
# All pods in the deployment will be restarted
kubectl -n odoo-prod rollout restart deployment


# Check the status of the Odoo deployment
kubectl -n odoo-prod rollout status deploy/odoo

# Logs from Postgres
kubectl -n odoo-prod logs deploy/db

# Logs from Odoo
kubectl -n odoo-prod logs deploy/odoo

# VERIFYING THE IMAGE BAKED SETUP
# Exec into the Odoo pod to check the odoo.conf file for admin_passwd
kubectl -n odoo-prod exec -it deploy/odoo -- cat /etc/odoo/odoo.conf | grep admin_passwd
# It should show: admin_passwd = adm

# Check port forwarding to access Odoo web interface locally
kubectl -n odoo-prod port-forward deploy/odoo 8069:8069
# Now access Odoo at http://localhost:8069 in your web browser

# Aceess Odoo Web Interface
http://4.253.33.191:8069/web/database/manager
http://4.253.33.191:8069

# TO DELETE OLD RESOURCES

# Delete PVC
kubectl delete pvc postgres-pvc -n odoo-prod

# To Delete All Pods in the Namespace
kubectl delete namespace odoo-prod

kubectl delete pods --all -n odoo-prod

# To Delete All services in the Namespace
kubectl delete svc --all -n odoo-prod

# To Delete All replicaset in the Namespace
kubectl delete rs --all -n odoo-prod
# To Delete All deployments in the Namespace
kubectl delete deploy --all -n odoo-prod

# before applying a new manifest... delete the below resources if exists
kubectl delete pvc postgres-pvc -n odoo-prod
kubectl -n odoo-prod delete configmap initdb-scripts
kubectl delete svc --all -n odoo-prod
kubectl delete rs --all -n odoo-prod
kubectl delete deploy --all -n odoo-prod

# To Delete Resources
kubectl delete -f dbms-aks.yaml
kubectl delete -f dbms-postgres.yaml

# Delete the old PostgreSQL deployment
kubectl -n odoo-prod delete deployment postgres
# Delete the old PostgreSQL service
kubectl -n odoo-prod delete service postgres-svc

# Delete the old Odoo deployment
kubectl -n odoo-prod delete deployment odoo
# Delete the old Odoo service
kubectl -n odoo-prod delete service odoo-svc

# hased admin password for odoo.conf: admin_passwd = admin
qebv-wawd-2fka

# STRESS TESTING THE ODOO DEPLOYMENT FOR 10,000 LOGINS USING K6
k6 run login-test.js

---


### Why stress testing matters for your setup

Stress testing tells you how your system behaves when demand exceeds design limits—where it breaks, how it degrades, and whether it recovers. For a DBMS/Odoo app with 10,000 registered users, it answers different questions than standard load testing: not just “can it handle expected traffic,” but “what happens when everyone shows up at once,” and “which component fails first—auth, database, network, or Kubernetes scaling.”  

---

### What to simulate for a school portal

- **Traffic patterns:** Simultaneous logins at the opening bell, rapid navigation between key pages (hostel booking), and bursty spikes far above normal.  
- **User behavior:** Short sessions that authenticate, fetch profile/booking pages, submit forms, and retry under errors; not just raw HTTP hits.  
- **Failure modes:** Timeouts, 5xx errors, rising latency, queueing at the DB or workers, and cascading failures (pods crash-looping, HPA flapping).  
- **Back-end constraints:** Postgres connection pool exhaustion, Odoo workers saturation, rate limits, ingress throttling, and disk/network I/O contention.  

You’re right to test “10k logins at once” and to watch metrics live; that’s exactly the bottleneck your institution experienced during hostel booking.  

---

### Open-source load tools for simulating 10k logins

| Tool | Concurrency model | Scenario scripting | Ease of setup | Best for |
| --- | --- | --- | --- | --- |
| k6 | VUs, ramping, thresholds | JavaScript | Very easy | High-concurrency HTTP auth flows |
| Locust | Python users/tasks | Python | Easy | Realistic user journeys |
| JMeter | Thread groups | GUI/XML | Medium | Complex protocols, broad plugins |

> Sources: 

These tools are widely used to generate controlled load and stress scenarios; they support ramp-ups, think times, custom headers/cookies, and assertions that match login success criteria. Listings and reviews commonly highlight k6, Locust, and JMeter among top open-source choices for load/stress testing in 2025.  

---

### Live metrics for Kubernetes and Azure

- **Prometheus + Grafana:** Scrape app/pod metrics (latency, throughput, errors), visualize dashboards, set alerts. Pair with kube-state-metrics for cluster health. These are among the top open-source choices for K8s monitoring and are frequently recommended for real-time visibility into pods, nodes, and workloads.  
- **Azure Monitor / Container Insights:** Node/pod CPU, memory, restarts, network, and live logs—helpful if you’re already in AKS.  
- **Logging/tracing:** Centralize logs (e.g., ELK) and add app-level tracing where possible to pinpoint slow spans under load.  

---

### Quick setup under 1 hour

#### 1) Prepare test accounts and environment
- **Credentials:** Your email pattern and shared passwords will work, but consider staggering logins to mimic realistic waves and avoid lockouts.  
- **Target:** Odoo base URL at your public IP and port. Verify CSRF/auth endpoints and any necessary cookies.

#### 2) Choose one tool and script the login flow
- **Option A: k6 (fastest to start)**

```javascript
import http from 'k6/http';
import { check, sleep } from 'k6';

export const options = {
  stages: [
    { duration: '1m', target: 1000 },
    { duration: '2m', target: 5000 },
    { duration: '2m', target: 10000 },
    { duration: '2m', target: 0 },
  ],
  thresholds: {
    http_req_duration: ['p(95)<2000'],
    http_req_failed: ['rate<0.05'],
  },
};

function creds(i) {
  if (i < 10000) {
    return { email: `student${i+1}@st.futminna.edu.ng`, password: 'student123' };
  } else {
    const j = i - 10000;
    return { email: `faculty${j+1}@st.futminna.edu.ng`, password: 'staff123' };
  }
}

export default function () {
  const i = (__VU - 1) % 10500;
  const { email, password } = creds(i);

  // 1) Get login page (capture cookies/CSRF if needed)
  let r1 = http.get('http://4.253.33.191:8069/web/login');
  check(r1, { 'login page 200': (r) => r.status === 200 });

  // 2) Post credentials (adjust fields to match Odoo)
  const payload = {
    login: email,
    password: password,
    redirect: '',
  };
  let r2 = http.post('http://4.253.33.191:8069/web/login', payload, {
    headers: { 'Content-Type': 'application/x-www-form-urlencoded' },
  });
  check(r2, {
    'logged in or redirected': (r) => r.status === 200 || r.status === 302,
  });

  // 3) Hit a booking page endpoint to simulate real usage
  let r3 = http.get('http://4.253.33.191:8069/hostel/booking');
  check(r3, { 'booking page ok': (r) => r.status === 200 });

  sleep(1);
}
```

- **Option B: Locust (more realistic journeys)**

```python
from locust import HttpUser, task, between
import random

def cred(index):
    if index < 10000:
        return f"student{index+1}@st.futminna.edu.ng", "student123"
    else:
        j = index - 10000
        return f"faculty{j+1}@st.futminna.edu.ng", "staff123"

class OdooUser(HttpUser):
    wait_time = between(0.5, 2.0)

    def on_start(self):
        i = random.randint(0, 10499)
        email, password = cred(i)
        with self.client.get("/web/login", name="login_page", catch_response=True) as r:
            if r.status_code != 200:
                r.failure("login page failed")

        payload = {"login": email, "password": password, "redirect": ""}
        with self.client.post("/web/login", data=payload, name="login_post", catch_response=True) as r:
            if r.status_code not in [200, 302]:
                r.failure("login failed")

    @task
    def booking(self):
        self.client.get("/hostel/booking", name="booking_page")
```

Both k6 and Locust are commonly cited among leading open-source tools for rapid load/stress testing in 2025, suitable for simulating high concurrency and user flows.  

#### 3) Run the test
- **k6:** `k6 run test.js` (scale VUs based on machine capacity; distribute across multiple runners if needed).  
- **Locust:** `locust -f locustfile.py --host http://4.253.33.191:8069` and ramp users to 10k from the web UI.

#### 4) Monitor in real time
- **Prometheus + Grafana:** If AKS has these installed, open Grafana dashboards for: pod CPU/memory, request latency, 5xx rate, restarts, HPA activity, DB connections. These tools are routinely recommended for Kubernetes cluster monitoring and performance visibility.  
- **Azure Monitor:** Check Container Insights for node/pod saturation, network throughput, and live logs.  

---

### Metrics to watch under stress

- **App-level:** p50/p95/p99 latency, throughput (RPS), error rate, login success ratio, queue lengths.  
- **Kubernetes:** Pod restarts, throttling, HPA scale events, node CPU/memory pressure, network saturation.  
- **Database (Postgres):** Active connections vs max, lock waits, slow queries, IOPS, buffer/cache hit ratio.  
- **Ingress/load balancer:** 4xx/5xx, connection counts, SYN backlog, TLS handshakes.  

---

### Interpreting results and next steps

- **If login collapses first:** Add worker processes, tune gunicorn/Odoo workers, increase DB pool, add caching for static flows, consider rate-limiting plus queuing for booking start.  
- **If DB is the bottleneck:** Raise max connections pragmatically, add pgbouncer, tune indexes/queries, scale storage IOPS.  
- **If K8s flaps:** Stabilize HPA (cooldowns), set resource requests/limits, use PodDisruptionBudgets, ensure adequate nodes, pre-scale before booking windows.  
- **If ingress fails:** Increase backend timeouts, connection limits, and enable keep-alives; consider horizontal scaling of the ingress controller.

---
### Summary
- **approach is right.** Simulating 10k concurrent logins and monitoring live is exactly how to surface the bottlenecks you observed (lag, crashes, broken connections).  
- **Under 1 hour is doable** if you pick one tool (k6 or Locust), start with a minimal script, and use existing AKS monitoring (Azure Monitor or pre-installed Prometheus/Grafana). Full observability setup from scratch can take longer, but basic testing plus core metrics is achievable in that timeframe.  

---

<!-- # login-test.js -->

```
import http from 'k6/http';
import { Counter, Trend } from 'k6/metrics';

// Custom metrics
export let loginSuccess = new Counter('login_success');
export let loginFail = new Counter('login_fail');
export let requestSuccess = new Counter('request_success');
export let requestFail = new Counter('request_fail');
export let under1min = new Counter('under_1min');
export let over1min = new Counter('over_1min');
export let latency = new Trend('latency', true);

export const options = {
  stages: [
    { duration: '30s', target: 100 },   // ramp up to 100 users
    { duration: '1m', target: 100 },    // hold steady at 100
    { duration: '30s', target: 0 },     // ramp down
  ],
};

function creds(i) {
  const j = i % 500;
  return {
    email: `faculty${j + 1}`,
    password: 'staff123',
  };
}

// Extract CSRF token from login page HTML
function extractCSRF(html) {
  if (!html) return null;
  const match = html.match(/name="csrf_token"[^>]*value="([^"]+)"/i);
  return match ? match[1] : null;
}

export default function () {
  const i = (__VU - 1);
  const { email, password } = creds(i);

  const jar = new http.CookieJar();

  // 1) Get login page
  let r1 = http.get('http://4.253.33.191:8069/web/login', { jar });
  if (r1.status !== 200 || !r1.body) {
    requestFail.add(1);
    loginFail.add(1);
    return; // skip this iteration if login page fails
  }
  requestSuccess.add(1);

  const csrf = extractCSRF(r1.body);
  if (!csrf) {
    requestFail.add(1);
    loginFail.add(1);
    return; // skip if CSRF token is missing
  }

  // 2) Post credentials with cookies + CSRF
  const form = {
    login: email,
    password: password,
    redirect: '',
    csrf_token: csrf,
  };

  const payload = Object.entries(form)
    .map(([k, v]) => `${encodeURIComponent(k)}=${encodeURIComponent(v)}`)
    .join('&');

  const start = Date.now();
  let r2 = http.post('http://4.253.33.191:8069/web/login', payload, {
    jar,
    headers: { 'Content-Type': 'application/x-www-form-urlencoded' },
    redirects: 0,
  });
  const duration = Date.now() - start;
  latency.add(duration);

  if (r2.status === 200 || r2.status === 302) {
    loginSuccess.add(1);
    requestSuccess.add(1);
  } else {
    loginFail.add(1);
    requestFail.add(1);
  }

  if (duration < 60000) {
    under1min.add(1);
  } else {
    over1min.add(1);
  }
}

---


<!-- # Manifest.yaml -->
---
apiVersion: v1
kind: Namespace
metadata:
  name: odoo-prod
---
# ConfigMap for Odoo configuration (optional override)
apiVersion: v1
kind: ConfigMap
metadata:
  name: odoo-config
  namespace: odoo-prod
data:
  odoo.conf: |
    [options]
    addons_path = /usr/lib/python3/dist-packages/odoo/addons,/mnt/extra-addons
    admin_passwd = admin
    db_host = db
    db_port = 5432
    db_user = odoo
    db_password = odoo
    list_db = True
    log_level = info
    http_port = 8069
    gevent_port = 8072
    server_wide_modules = base,web
    workers = 4
    csv_internal_sep = ,
    data_dir = /var/lib/odoo/sessions
    geoip_city_db = /usr/share/GeoIP/GeoLite2-City.mmdb
    geoip_country_db = /usr/share/GeoIP/GeoLite2-Country.mmdb
    screenshots = /tmp/odoo_tests
---
# PostgreSQL Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: db
  namespace: odoo-prod
spec:
  replicas: 1
  selector:
    matchLabels:
      app: db
  template:
    metadata:
      labels:
        app: db
    spec:
      containers:
        - name: postgres
          image: postgres:17.5
          ports:
            - containerPort: 5432
          env:
            - name: POSTGRES_USER
              value: postgres
            - name: POSTGRES_PASSWORD
              value: postgres
          volumeMounts:
            - name: pgdata
              mountPath: /var/lib/postgresql/data
            - name: initdb
              mountPath: /docker-entrypoint-initdb.d
          readinessProbe:
            exec:
              command: ["pg_isready", "-U", "odoo", "-d", "postgres"]
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 3
            failureThreshold: 5
      volumes:
        - name: pgdata
          emptyDir: {}
        - name: initdb
          configMap:
            name: initdb-scripts
---
# PostgreSQL Service
apiVersion: v1
kind: Service
metadata:
  name: db
  namespace: odoo-prod
spec:
  selector:
    app: db
  ports:
    - port: 5432
      targetPort: 5432
---
# Odoo Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: odoo
  namespace: odoo-prod
spec:
  replicas: 1
  selector:
    matchLabels:
      app: odoo
  template:
    metadata:
      labels:
        app: odoo
    spec:
      initContainers:
        - name: wait-for-postgres
          image: postgres:17.5
          command: ['sh', '-c', 'until pg_isready -h db -U postgres; do echo waiting for db; sleep 2; done;']
      containers:
        - name: odoo
          image: legacyg/dbmsdocker:with-addons-v1
          ports:
            - containerPort: 8069
            - containerPort: 8072
          env:
            - name: HOST
              value: db
            - name: USER
              value: odoo
            - name: PASSWORD
              value: odoo
          volumeMounts:
            # Optional: override baked-in config with ConfigMap
            - name: config
              mountPath: /etc/odoo/odoo.conf
              subPath: odoo.conf
      volumes:
        - name: config
          configMap:
            name: odoo-config
---
# Odoo Service
apiVersion: v1
kind: Service
metadata:
  name: odoo-svc
  namespace: odoo-prod
spec:
  type: LoadBalancer
  selector:
    app: odoo
  ports:
    - name: http
      port: 8069
      targetPort: 8069
    - name: longpoll
      port: 8072
      targetPort: 8072
---